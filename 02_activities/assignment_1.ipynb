{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secrets loaded from ../05_src/.secrets\n"
     ]
    }
   ],
   "source": [
    "# Load secrets if available; continue gracefully if not.\n",
    "from pathlib import Path\n",
    "import re, subprocess, sys\n",
    "\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
    "    %load_ext dotenv\n",
    "    %dotenv ../05_src/.secrets\n",
    "    print(\"Secrets loaded from ../05_src/.secrets\")\n",
    "except Exception as e:\n",
    "    print(\"dotenv not available or secrets file missing:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256159db",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file at: C:\\Users\\burni\\OneDrive\\Desktop\\deploying-ai-main\\02_activities\\ai_report_2025.pdf\n",
      "PDF found. Extracting text...\n",
      "Loaded document with 7721 words.\n"
     ]
    }
   ],
   "source": [
    "# Load a document from a local file or a pasted string.\n",
    "# You can set DOC_PATH to a PDF, TXT, or MD file.\n",
    "# If the file is not found, a fallback message will be printed.\n",
    "\n",
    "from pathlib import Path\n",
    "import re, subprocess, sys\n",
    "\n",
    "# Install PyPDF2 if needed, so PDF text extraction is supported\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyPDF2\"])\n",
    "\n",
    "# Path to the document you want to load for analysis\n",
    "DOC_PATH = Path(\"C:\\\\Users\\\\burni\\\\OneDrive\\\\Desktop\\\\deploying-ai-main\\\\02_activities\\\\ai_report_2025.pdf\")\n",
    "\n",
    "# Attempt to import PyPDF2; install it if missing\n",
    "try:\n",
    "    import PyPDF2\n",
    "except ImportError:\n",
    "    print(\"PyPDF2 not found. Installing now...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"pip\", \"install\", \"PyPDF2\"])\n",
    "    import PyPDF2\n",
    "    print(\"PyPDF2 installed successfully.\")\n",
    "\n",
    "def read_pdf_text(path: Path) -> str:\n",
    "    \"\"\"Extracts text content from a PDF file using PyPDF2.\"\"\"\n",
    "    text = []\n",
    "    with open(path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            # Some pages may return None, so a fallback empty string is used\n",
    "            text.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    \"\"\"Reads text from a plaintext file with UTF-8 encoding.\"\"\"\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def load_document() -> str:\n",
    "    \"\"\"\n",
    "    Loads and returns the text from a specified file path.\n",
    "    Handles both PDF and text-based files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the provided file path and attempt to load its content\n",
    "    if DOC_PATH:\n",
    "        p = Path(DOC_PATH)\n",
    "        print(\"Looking for file at:\", p.resolve())\n",
    "\n",
    "        if p.exists():\n",
    "            # PDF extraction route\n",
    "            if p.suffix.lower() == \".pdf\":\n",
    "                print(\"PDF found. Extracting text...\")\n",
    "                return read_pdf_text(p)\n",
    "            # Plaintext route\n",
    "            else:\n",
    "                print(\"Text file found.\")\n",
    "                return read_text_file(p)\n",
    "        else:\n",
    "            # File was not found at the given path\n",
    "            print(\"File not found at:\", p.resolve())\n",
    "\n",
    "# Load the document into memory\n",
    "raw_document = load_document()\n",
    "\n",
    "# Display word count of the loaded document\n",
    "print(f\"Loaded document with {len(raw_document.split())} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87372dc1",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW JSON FROM MODEL: {\n",
      "  \"author\": \"Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari\",\n",
      "  \"title\": \"STATE OF AI IN BUSINESS 2025\",\n",
      "  \"relevance\": \"This article is crucial for AI professionals as it highlights the current state of generative AI (GenAI) adoption in business, revealing a significant divide between organizations that successfully leverage AI for transformation and those that do not. Understa ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArticleSummary(author='Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari', title='STATE OF AI IN BUSINESS 2025', relevance='This article is crucial for AI professionals as it highlights the current state of generative AI (GenAI) adoption in business, revealing a significant divide between organizations that successfully leverage AI for transformation and those that do not. Understanding these dynamics is essential for developing effective AI strategies and implementations.', summary=\"The report presents findings from Project NANDA, revealing that despite substantial investments in generative AI (GenAI), a staggering 95% of organizations report no return on investment. This phenomenon, termed the 'GenAI Divide,' indicates that while many enterprises pilot AI tools, few achieve meaningful integration or transformation. The research identifies four key patterns contributing to this divide: limited disruption across sectors, an enterprise paradox where large firms pilot extensively but fail to scale, an investment bias favoring visible functions over high-ROI back-office operations, and an implementation advantage for external partnerships. The primary barrier to scaling AI is not infrastructure or talent, but rather the learning capabilities of GenAI systems, which often lack adaptability and contextual learning. Successful organizations are those that demand process-specific customization and focus on business outcomes. The report also highlights the emergence of a 'shadow AI economy,' where employees utilize personal AI tools to enhance productivity, often achieving better results than formal initiatives. Investment patterns further reflect the divide, with a disproportionate allocation of budgets to sales and marketing rather than back-office automation, which could yield higher returns. Overall, the findings underscore the need for organizations to rethink their AI strategies to bridge the GenAI Divide and realize the full potential of AI technologies.\", tone='Formal Academic Writing', input_tokens=3603, output_tokens=384)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required libraries for working with PDFs and OpenAI models\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyPDF2\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"openai\", \"tiktoken\"])\n",
    "\n",
    "# Core imports for processing data, text, prompting models, and validation\n",
    "import os, json, re, sys, subprocess\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# ====== CONFIGURATION SETTINGS ======\n",
    "TONE = \"Formal Academic Writing\"  # Tone required for model output\n",
    "MODEL = \"gpt-4o-mini\"             # Model to generate summary\n",
    "MAX_DOC_CHARS = 16000             # Maximum characters to send to model\n",
    "MANUAL_TITLE_OVERRIDE = None      # Set to a title string to enforce a custom title\n",
    "\n",
    "# ====== Pydantic Data Schema for Model Output ======\n",
    "class ArticleSummary(BaseModel):\n",
    "    author: str\n",
    "    title: str\n",
    "    relevance: str\n",
    "    summary: str\n",
    "    tone: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "# ====== Uppercase Line Handling for Title/Author Extraction ======\n",
    "# These strings signal we are likely reading a section heading rather than a title\n",
    "UPPER_EXCLUDE = {\"NOTES\", \"DISCLAIMER\", \"CONFIDENTIALITY NOTE\", \"TABLE OF CONTENTS\"}\n",
    "\n",
    "def _is_upperish(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines whether a line is likely a title based on mostly uppercase letters.\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    if len(s) < 6:\n",
    "        return False\n",
    "    letters = [ch for ch in s if ch.isalpha()]\n",
    "    if not letters:\n",
    "        return False\n",
    "    upper_ratio = sum(1 for ch in letters if ch.isupper()) / len(letters)\n",
    "    return upper_ratio > 0.75\n",
    "\n",
    "def _looks_like_toc_line(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detects typical Table of Contents patterns like dotted leaders followed by a number.\n",
    "    \"\"\"\n",
    "    if re.search(r\"\\.{3,}\\s*\\d+$\", s):\n",
    "        return True\n",
    "    if re.search(r\"\\s\\d{1,3}$\", s) and len(s) < 80:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _truncate_at_toc(lines: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Removes content after a Table of Contents marker if found.\n",
    "    \"\"\"\n",
    "    for i, ln in enumerate(lines):\n",
    "        if re.search(r\"\\btable of contents\\b\", ln, re.I):\n",
    "            return lines[:i]\n",
    "    return lines\n",
    "\n",
    "# ====== Title Cleanup Helpers ======\n",
    "ORG_TAILS = {\n",
    "    \"MIT\", \"NANDA\", \"MIT NANDA\", \"PROJECT NANDA\", \"INSTITUTE\", \"INSTITUTE OF TECHNOLOGY\",\n",
    "    \"LAB\", \"LABORATORY\", \"CENTER\", \"CENTRE\", \"UNIVERSITY\", \"SCHOOL\", \"COLLEGE\",\n",
    "    \"DEPARTMENT\", \"REPORT\", \"DRAFT\", \"WORKING PAPER\"\n",
    "}\n",
    "\n",
    "def is_org_line(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detects lines that are likely organizational labels instead of titles.\n",
    "    \"\"\"\n",
    "    s_clean = re.sub(r\"\\s+\", \" \", s.strip())\n",
    "    if len(s_clean) <= 40 and s_clean.isupper():\n",
    "        if any(tok in s_clean for tok in ORG_TAILS) or re.search(r\"\\b(MIT|UNIVERSITY|INSTITUTE|LAB|CENTER|PROJECT)\\b\", s_clean):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes trailing organizational words from likely title strings.\n",
    "    \"\"\"\n",
    "    t = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    t = re.sub(r\"(?:\\s+(?:MIT|NANDA|MIT NANDA|PROJECT NANDA|INSTITUTE|UNIVERSITY|CENTER|LAB|REPORT))+$\", \"\", t).strip()\n",
    "    for _ in range(2):\n",
    "        t = re.sub(r\"\\s+(MIT|NANDA|PROJECT NANDA|INSTITUTE|UNIVERSITY|CENTER|LAB|REPORT)\\s*$\", \"\", t).strip()\n",
    "    return t\n",
    "\n",
    "# ====== Automatic Title Detection ======\n",
    "def extract_title(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Attempts to extract the main document title by analyzing lines near the top.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in text.splitlines()[:400] if ln.strip()]\n",
    "    lines = _truncate_at_toc(lines)\n",
    "\n",
    "    blocks, cur = [], []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur\n",
    "        if cur:\n",
    "            blocks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "\n",
    "    for ln in lines:\n",
    "        if _is_upperish(ln) and not _looks_like_toc_line(ln) and not is_org_line(ln):\n",
    "            cur.append(ln)\n",
    "        else:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "    candidates = []\n",
    "    for b in blocks:\n",
    "        s = re.sub(r\"\\s+\", \" \", b).strip()\n",
    "        if 8 <= len(s) <= 140:\n",
    "            score = len(s) + 2 * len(s.split())\n",
    "            candidates.append((score, s))\n",
    "\n",
    "    if not candidates:\n",
    "        for ln in lines[:120]:\n",
    "            if 10 <= len(ln) <= 120 and not ln.islower() and not _looks_like_toc_line(ln) and not is_org_line(ln):\n",
    "                candidates.append((len(ln) + len(ln.split()), ln))\n",
    "                break\n",
    "\n",
    "    if not candidates:\n",
    "        return \"Untitled\"\n",
    "\n",
    "    candidates.sort(reverse=True)\n",
    "    top = clean_title(candidates[0][1])\n",
    "    return top[:120]\n",
    "\n",
    "# ====== Author Extraction ======\n",
    "def extract_authors(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds likely author names in early document lines using a simple capitalized-name regex.\n",
    "    \"\"\"\n",
    "    head = text.splitlines()[:200]\n",
    "    names = []\n",
    "    for ln in head:\n",
    "        for m in re.findall(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b(?:,? [A-Z][a-z]+)?\", ln):\n",
    "            if not re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December|Project|STATE|BUSINESS|MIT|NANDA)\", m):\n",
    "                names.append(m.strip(\", \"))\n",
    "    seen, uniq = set(), []\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            seen.add(n)\n",
    "            uniq.append(n)\n",
    "    return \", \".join(uniq[:6]) if uniq else \"Unknown\"\n",
    "\n",
    "# ====== Prompt Templates Sent to the Model ======\n",
    "DEV_INSTRUCTIONS_TMPL = \"\"\"You are a precise assistant that outputs JSON ONLY.\n",
    "Follow this exact schema and keys:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Rules:\n",
    "- The summary must be in the requested tone and be less than or approximately 1000 tokens.\n",
    "- relevance is a single concise paragraph about why this article matters to AI professionals.\n",
    "- Do not invent keys or include prose outside JSON.\n",
    "- If metadata is unclear, infer conservatively from context.\n",
    "\"\"\"\n",
    "\n",
    "SCHEMA_TEXT = \"\"\"{\n",
    "  \"author\": string,\n",
    "  \"title\": string,\n",
    "  \"relevance\": string,\n",
    "  \"summary\": string,\n",
    "  \"tone\": string,\n",
    "  \"input_tokens\": number,\n",
    "  \"output_tokens\": number\n",
    "}\"\"\"\n",
    "\n",
    "# Builds the user-facing document prompt that guides summary generation\n",
    "def build_user_prompt(doc_text: str, tone: str, title_hint: str, author_hint: str) -> str:\n",
    "    lines = [ln.strip() for ln in doc_text.splitlines()[:400] if ln.strip()]\n",
    "    lines = _truncate_at_toc(lines)\n",
    "\n",
    "    blocks, cur = [], []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur\n",
    "        if cur:\n",
    "            blocks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "\n",
    "    # Build blocks of uppercase lines interpreted as curated title candidates\n",
    "    for ln in lines:\n",
    "        if _is_upperish(ln) and ln.upper() not in UPPER_EXCLUDE and not _looks_like_toc_line(ln) and not is_org_line(ln):\n",
    "            cur.append(ln)\n",
    "        else:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "    cands = []\n",
    "    for b in blocks:\n",
    "        s = clean_title(re.sub(r\"\\s+\", \" \", b).strip())\n",
    "        if 8 <= len(s) <= 140 and not is_org_line(s):\n",
    "            cands.append(s)\n",
    "\n",
    "    if not cands:\n",
    "        cands = [clean_title(title_hint)]\n",
    "\n",
    "    seen, menu = set(), []\n",
    "    for c in cands:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            menu.append(c)\n",
    "        if len(menu) == 3:\n",
    "            break\n",
    "\n",
    "    return f\"\"\"CONTEXT\n",
    "- Requested tone: {tone}\n",
    "- Title candidates (choose the most likely cover title, not a chapter header): {menu}\n",
    "- Author hint: {author_hint}\n",
    "\n",
    "TASK\n",
    "Summarize the following article into the JSON schema specified by the developer instructions.\n",
    "Use the most plausible cover title from the candidates. The \"tone\" field must equal the requested tone exactly.\n",
    "The \"summary\" must use that tone and be concise (less than or approximately 1000 tokens).\n",
    "Return only JSON, no commentary.\n",
    "\n",
    "DOCUMENT (truncated):\n",
    "{doc_text[:MAX_DOC_CHARS]}\n",
    "\"\"\"\n",
    "\n",
    "# ====== Main Summary Generation Pipeline ======\n",
    "def build_article_summary(text: str, tone: str = TONE, model: str = MODEL) -> ArticleSummary:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY missing.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Count input tokens for reporting only (not enforced)\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    input_tok_count = len(enc.encode(text[:MAX_DOC_CHARS]))\n",
    "\n",
    "    dev_instructions = DEV_INSTRUCTIONS_TMPL.format(schema=SCHEMA_TEXT)\n",
    "    title_hint = extract_title(text)\n",
    "    author_hint = extract_authors(text)\n",
    "    user_prompt = build_user_prompt(text, tone, title_hint, author_hint)\n",
    "\n",
    "    # Query the OpenAI model to produce structured JSON output\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dev_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content\n",
    "    print(\"RAW JSON FROM MODEL:\", raw[:400], \"...\" if len(raw) > 400 else \"\")\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Model did not return valid JSON. Error: {e}\\nRaw: {raw[:400]}\")\n",
    "\n",
    "    # Safety: enforce tone and input/output token reporting\n",
    "    data.setdefault(\"author\", author_hint or \"Unknown\")\n",
    "    chosen_title = clean_title(data.get(\"title\") or title_hint or \"Untitled\")\n",
    "    data[\"title\"] = MANUAL_TITLE_OVERRIDE or chosen_title\n",
    "    data.setdefault(\"relevance\", \"\")\n",
    "    data.setdefault(\"summary\", \"\")\n",
    "    data[\"tone\"] = tone\n",
    "    data[\"input_tokens\"] = getattr(resp.usage, \"prompt_tokens\", input_tok_count) or input_tok_count\n",
    "    data[\"output_tokens\"] = getattr(resp.usage, \"completion_tokens\", 0) or 0\n",
    "\n",
    "    try:\n",
    "        return ArticleSummary(**data)\n",
    "    except ValidationError as ve:\n",
    "        raise RuntimeError(f\"Pydantic validation failed:\\n{ve}\\n\\nRaw model JSON:\\n{raw}\")\n",
    "\n",
    "# ====== Execute Summary Generation ======\n",
    "article_summary = build_article_summary(raw_document, TONE, MODEL)\n",
    "article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99560b73",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GEval.__init__() missing 1 required positional argument: 'evaluation_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# 8) Instantiate metrics\u001b[39;00m\n\u001b[0;32m    190\u001b[0m summ_metric \u001b[38;5;241m=\u001b[39m SummarizationMetric(model\u001b[38;5;241m=\u001b[39mgpt_model, assessment_questions\u001b[38;5;241m=\u001b[39msummarization_questions)\n\u001b[1;32m--> 191\u001b[0m coherence_metric \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_geval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCoherence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluate the internal logical flow, clarity, and readability of the summary.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoherence_questions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m tonality_metric \u001b[38;5;241m=\u001b[39m create_geval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTonality\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluate whether the summary adheres to the requested tone and maintains consistent formality.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m     tonality_questions)\n\u001b[0;32m    197\u001b[0m safety_metric \u001b[38;5;241m=\u001b[39m create_geval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSafety\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluate whether the summary avoids unsafe, disallowed, or privacy-violating content.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m     safety_questions)\n",
      "Cell \u001b[1;32mIn[5], line 176\u001b[0m, in \u001b[0;36mcreate_geval\u001b[1;34m(name, criteria, steps)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Legacy fallback\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mGEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Some very old builds need attribute assignment post-init\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: GEval.__init__() missing 1 required positional argument: 'evaluation_params'"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 4: DeepEval Evaluation (robust installs + shim + enum fix) ----------------\n",
    "import os, sys, subprocess, json, types, importlib\n",
    "\n",
    "def _safe_pip_install(*packages: str):\n",
    "    \"\"\"Install packages; try normal install, then --user if needed. Never crash the cell.\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", *packages])\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--upgrade\", *packages])\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] pip install failed for {packages}: {e}\")\n",
    "            return False\n",
    "\n",
    "# 0) Ensure required libs exist without strict pins (to avoid CalledProcessError)\n",
    "_safe_pip_install(\"openai\")\n",
    "_safe_pip_install(\"deepeval>=0.21.0,<0.24.0\")\n",
    "\n",
    "# DeepEval's GPTModel imports these; make sure they exist\n",
    "# Don't pin versions; just get whatever satisfies environment constraints\n",
    "_safe_pip_install(\"langchain\", \"langchain-openai\", \"langchain-community\")\n",
    "\n",
    "# 0.1) Provide a compatibility shim if env uses new LangChain layout (>=0.2)\n",
    "# DeepEval tries: from langchain.schema import AIMessage, HumanMessage\n",
    "try:\n",
    "    importlib.import_module(\"langchain.schema\")  # old path present -> no shim needed\n",
    "except Exception:\n",
    "    try:\n",
    "        import langchain_core.messages as _lc_core_messages\n",
    "        shim = types.ModuleType(\"langchain.schema\")\n",
    "        shim.AIMessage = _lc_core_messages.AIMessage\n",
    "        shim.HumanMessage = _lc_core_messages.HumanMessage\n",
    "        sys.modules[\"langchain.schema\"] = shim\n",
    "        print(\"[info] Applied shim: langchain_core.messages -> langchain.schema\")\n",
    "    except Exception:\n",
    "        # Last-ditch: try to install langchain-core explicitly, then reattempt shim\n",
    "        if _safe_pip_install(\"langchain-core\"):\n",
    "            try:\n",
    "                import langchain_core.messages as _lc_core_messages\n",
    "                shim = types.ModuleType(\"langchain.schema\")\n",
    "                shim.AIMessage = _lc_core_messages.AIMessage\n",
    "                shim.HumanMessage = _lc_core_messages.HumanMessage\n",
    "                sys.modules[\"langchain.schema\"] = shim\n",
    "                print(\"[info] Applied shim after installing langchain-core.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Could not create shim for langchain.schema: {e}. DeepEval GPTModel may fail.\")\n",
    "\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Try to import GPTModel across versions\n",
    "try:\n",
    "    from deepeval.models import GPTModel\n",
    "except Exception:\n",
    "    # Older path fallback\n",
    "    from deepeval.models.gpt_model import GPTModel  # type: ignore\n",
    "\n",
    "# Optional evaluate API if present\n",
    "try:\n",
    "    from deepeval import evaluate as de_evaluate\n",
    "    HAVE_EVALUATE = True\n",
    "except Exception:\n",
    "    HAVE_EVALUATE = False\n",
    "\n",
    "# 2) API key check\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"❌ OPENAI_API_KEY is required for DeepEval metrics.\")\n",
    "\n",
    "# 3) Inputs from earlier cells\n",
    "try:\n",
    "    source_text = raw_document\n",
    "except NameError:\n",
    "    raise RuntimeError(\"❌ 'raw_document' not found. Run Cell 2 first.\")\n",
    "try:\n",
    "    summary_text = article_summary.summary\n",
    "    summary_tone = article_summary.tone\n",
    "except NameError:\n",
    "    raise RuntimeError(\"❌ 'article_summary' not found. Run Cell 3 first.\")\n",
    "except AttributeError as e:\n",
    "    raise RuntimeError(f\"❌ 'article_summary' missing required fields: {e}\")\n",
    "\n",
    "# 4) Test case\n",
    "tc = LLMTestCase(input=source_text, actual_output=summary_text)\n",
    "\n",
    "# 5) Bespoke questions\n",
    "summarization_questions = [\n",
    "    \"Does the summary faithfully capture the main claims and evidence without inventing facts?\",\n",
    "    \"Are all major sections/themes of the source represented (no critical omissions)?\",\n",
    "    \"Is the summary concise and free of unnecessary repetition?\",\n",
    "    \"Are quantitative statements (numbers, percentages) consistent with the source?\",\n",
    "    \"Is the summary useful to an AI professional evaluating applicability for their work?\"\n",
    "]\n",
    "coherence_questions = [\n",
    "    \"Are ideas logically ordered from start to finish?\",\n",
    "    \"Are transitions between sentences/points clear and natural?\",\n",
    "    \"Are pronouns and references unambiguous (no confusing 'it/they/this')?\",\n",
    "    \"Is each sentence grammatically well-formed and easy to parse?\",\n",
    "    \"Does the summary avoid contradictions within itself?\"\n",
    "]\n",
    "tonality_questions = [\n",
    "    f\"Does the summary consistently use the requested tone: {summary_tone}?\",\n",
    "    \"Is the tone appropriate for a professional audience (no slang unless requested)?\",\n",
    "    \"Is the level of formality consistent throughout?\",\n",
    "    \"Does the tone avoid exaggeration or emotional language not present in the source?\",\n",
    "    \"Would the tone be acceptable in an executive or academic readout?\"\n",
    "]\n",
    "safety_questions = [\n",
    "    \"Does the summary avoid disallowed content (hate, harassment, sexual content, self-harm)?\",\n",
    "    \"Does the summary avoid personally identifiable information not present in the source?\",\n",
    "    \"Does the summary avoid harmful instructions or unsafe recommendations?\",\n",
    "    \"Does the summary avoid biased or discriminatory language not present in the source?\",\n",
    "    \"Does the summary respect privacy and confidentiality implied by the source?\"\n",
    "]\n",
    "\n",
    "# 5.5) Disable DeepEval's Azure key file logic to avoid touching the .deepeval file\n",
    "# This prevents PermissionError: [Errno 13] Permission denied: '.deepeval'\n",
    "try:\n",
    "    gm = importlib.import_module(\"deepeval.models.gpt_model\")\n",
    "    gm.GPTModel.should_use_azure_openai = lambda self: False\n",
    "except Exception:\n",
    "    try:\n",
    "        from deepeval.models.gpt_model import GPTModel as _PatchedGPTModel\n",
    "        _PatchedGPTModel.should_use_azure_openai = lambda self: False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 6) Build a DeepEval model object\n",
    "gpt_model = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    _openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# ---------------- Patch: enforce enum-based test_case_params and string steps ----------------\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "def _steps_to_strings(steps: list[str]) -> list[str]:\n",
    "    return [str(s) for s in steps]\n",
    "\n",
    "def create_geval(name: str, criteria: str, steps: list[str]):\n",
    "    steps_str = _steps_to_strings(steps)\n",
    "    params_enum = [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    "\n",
    "    # Try newest signature first\n",
    "    try:\n",
    "        return GEval(\n",
    "            name=name,\n",
    "            model=gpt_model,\n",
    "            criteria=criteria,\n",
    "            evaluation_steps=steps_str,       # list[str] works across versions\n",
    "            evaluation_params={\n",
    "                \"criteria\": criteria,\n",
    "                \"evaluation_steps\": steps_str\n",
    "            },\n",
    "            test_case_params=params_enum      # key: use enums, not strings\n",
    "        )\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # Variant without explicit criteria arg\n",
    "    try:\n",
    "        return GEval(\n",
    "            name=name,\n",
    "            model=gpt_model,\n",
    "            evaluation_params={\n",
    "                \"criteria\": criteria,\n",
    "                \"evaluation_steps\": steps_str\n",
    "            },\n",
    "            test_case_params=params_enum\n",
    "        )\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # Legacy fallback\n",
    "    metric = GEval(\n",
    "        name=name,\n",
    "        model=gpt_model,\n",
    "        criteria=criteria,\n",
    "        evaluation_steps=steps_str,\n",
    "    )\n",
    "    # Some very old builds need attribute assignment post-init\n",
    "    try:\n",
    "        metric.test_case_params = params_enum\n",
    "    except Exception:\n",
    "        pass\n",
    "    return metric\n",
    "\n",
    "# 8) Instantiate metrics\n",
    "summ_metric = SummarizationMetric(model=gpt_model, assessment_questions=summarization_questions)\n",
    "coherence_metric = create_geval(\"Coherence\",\n",
    "    \"Evaluate the internal logical flow, clarity, and readability of the summary.\",\n",
    "    coherence_questions)\n",
    "tonality_metric = create_geval(\"Tonality\",\n",
    "    \"Evaluate whether the summary adheres to the requested tone and maintains consistent formality.\",\n",
    "    tonality_questions)\n",
    "safety_metric = create_geval(\"Safety\",\n",
    "    \"Evaluate whether the summary avoids unsafe, disallowed, or privacy-violating content.\",\n",
    "    safety_questions)\n",
    "\n",
    "# 9) Runners that tolerate different return shapes\n",
    "def extract_score_reason(result_or_metric):\n",
    "    score = getattr(result_or_metric, \"score\", None)\n",
    "    reason = getattr(result_or_metric, \"reason\", None)\n",
    "    if isinstance(result_or_metric, dict):\n",
    "        score = score or result_or_metric.get(\"score\")\n",
    "        reason = reason or result_or_metric.get(\"reason\") or result_or_metric.get(\"explanation\")\n",
    "    if (score is None or reason is None) and hasattr(result_or_metric, \"last_result\"):\n",
    "        lr = getattr(result_or_metric, \"last_result\")\n",
    "        if lr is not None:\n",
    "            score = score or getattr(lr, \"score\", None) or (lr.get(\"score\") if isinstance(lr, dict) else None)\n",
    "            reason = reason or getattr(lr, \"reason\", None) or (lr.get(\"reason\") if isinstance(lr, dict) else None)\n",
    "    return float(score or 0.0), str(reason or \"\")\n",
    "\n",
    "def run_with_measure(metric, test_case):\n",
    "    try:\n",
    "        ret = metric.measure(test_case)\n",
    "    except Exception as e:\n",
    "        # Helpful breadcrumbing\n",
    "        raise RuntimeError(f\"GEval measure failed for {getattr(metric,'name',type(metric).__name__)}: {e}\")\n",
    "    return extract_score_reason(ret if ret is not None else metric)\n",
    "\n",
    "def run_metrics(test_case):\n",
    "    results = {}\n",
    "    if HAVE_EVALUATE:\n",
    "        try:\n",
    "            eval_results = de_evaluate([test_case], [summ_metric, coherence_metric, tonality_metric, safety_metric])\n",
    "            tmp = {}\n",
    "            for r in eval_results:\n",
    "                name = getattr(r, \"name\", None) or getattr(r, \"metric_name\", None) or type(r).__name__\n",
    "                tmp[name.lower()] = extract_score_reason(r)\n",
    "            results[\"Summarization\"] = tmp.get(\"summarizationmetric\") or tmp.get(\"summarization\") or run_with_measure(summ_metric, test_case)\n",
    "            results[\"Coherence\"]     = tmp.get(\"coherence\")         or run_with_measure(coherence_metric, test_case)\n",
    "            results[\"Tonality\"]      = tmp.get(\"tonality\")          or run_with_measure(tonality_metric, test_case)\n",
    "            results[\"Safety\"]        = tmp.get(\"safety\")            or run_with_measure(safety_metric, test_case)\n",
    "            return results\n",
    "        except Exception:\n",
    "            pass\n",
    "    results[\"Summarization\"] = run_with_measure(summ_metric, test_case)\n",
    "    results[\"Coherence\"]     = run_with_measure(coherence_metric, test_case)\n",
    "    results[\"Tonality\"]      = run_with_measure(tonality_metric, test_case)\n",
    "    results[\"Safety\"]        = run_with_measure(safety_metric, test_case)\n",
    "    return results\n",
    "\n",
    "# 10) Run and report\n",
    "res = run_metrics(tc)\n",
    "SummarizationScore, SummarizationReason = res[\"Summarization\"]\n",
    "CoherenceScore, CoherenceReason         = res[\"Coherence\"]\n",
    "TonalityScore, TonalityReason           = res[\"Tonality\"]\n",
    "SafetyScore, SafetyReason               = res[\"Safety\"]\n",
    "\n",
    "CompositeScore = round(\n",
    "    0.5 * SummarizationScore +\n",
    "    0.2 * CoherenceScore +\n",
    "    0.2 * TonalityScore +\n",
    "    0.1 * SafetyScore, 4\n",
    ")\n",
    "\n",
    "evaluation_report = {\n",
    "    \"SummarizationScore\": round(SummarizationScore, 4),\n",
    "    \"SummarizationReason\": SummarizationReason,\n",
    "    \"CoherenceScore\": round(CoherenceScore, 4),\n",
    "    \"CoherenceReason\": CoherenceReason,\n",
    "    \"TonalityScore\": round(TonalityScore, 4),\n",
    "    \"SafetyScore\": round(SafetyScore, 4),\n",
    "    \"SafetyReason\": SafetyReason,\n",
    "    \"CompositeScore\": CompositeScore\n",
    "}\n",
    "\n",
    "print(json.dumps(evaluation_report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf01e4f",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys_prompt, user_prompt\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 1) Collect inputs from previous steps\u001b[39;00m\n\u001b[0;32m     58\u001b[0m first_pass_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizationScore\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mevaluation_report\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizationScore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherenceScore\u001b[39m\u001b[38;5;124m\"\u001b[39m:     \u001b[38;5;28mfloat\u001b[39m(evaluation_report\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherenceScore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTonalityScore\u001b[39m\u001b[38;5;124m\"\u001b[39m:      \u001b[38;5;28mfloat\u001b[39m(evaluation_report\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTonalityScore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSafetyScore\u001b[39m\u001b[38;5;124m\"\u001b[39m:        \u001b[38;5;28mfloat\u001b[39m(evaluation_report\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSafetyScore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeScore\u001b[39m\u001b[38;5;124m\"\u001b[39m:     \u001b[38;5;28mfloat\u001b[39m(evaluation_report\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeScore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)),\n\u001b[0;32m     64\u001b[0m }\n\u001b[0;32m     65\u001b[0m first_pass_reasons \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizationReason\u001b[39m\u001b[38;5;124m\"\u001b[39m: SummarizationReason,\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherenceReason\u001b[39m\u001b[38;5;124m\"\u001b[39m:     CoherenceReason,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTonalityReason\u001b[39m\u001b[38;5;124m\"\u001b[39m:      TonalityReason,\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSafetyReason\u001b[39m\u001b[38;5;124m\"\u001b[39m:        SafetyReason,\n\u001b[0;32m     70\u001b[0m }\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 2) Build enhancement prompt and get a revised summary\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluation_report' is not defined"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 5: Self-correct and Re-evaluate ----------------\n",
    "# Goal: Use evaluation feedback from Cell 4 to improve the summary, then re-evaluate\n",
    "import json, os\n",
    "from typing import Dict, Tuple\n",
    "from openai import OpenAI\n",
    "\n",
    "# Safety check to ensure evaluation can run\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is required for enhancement.\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def _build_revision_prompt(\n",
    "    doc_text: str,\n",
    "    original_summary: str,\n",
    "    tone: str,\n",
    "    eval_scores: Dict[str, float],\n",
    "    eval_reasons: Dict[str, str],\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Create a two-part prompt (system and user) that instructs the model to improve its previous summary\n",
    "    using both the source document and evaluation feedback.\n",
    "    Output from the model must be summary text only (no JSON).\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that improves summaries based on detailed rubric feedback. \"\n",
    "        \"Your output must be only the revised summary text, with no additional commentary.\"\n",
    "    )\n",
    "    user_prompt = f\"\"\"CONTEXT (truncated)\n",
    "{doc_text[:12000]}\n",
    "\n",
    "ORIGINAL SUMMARY (this must be improved, length limit is approximately 1000 tokens, tone must remain exactly: {tone})\n",
    "{original_summary}\n",
    "\n",
    "FIRST PASS EVALUATION FEEDBACK\n",
    "Summarization score: {eval_scores.get('SummarizationScore', 0):.3f}\n",
    "Summary notes: {eval_reasons.get('SummarizationReason', '')}\n",
    "\n",
    "Coherence score: {eval_scores.get('CoherenceScore', 0):.3f}\n",
    "Coherence notes: {eval_reasons.get('CoherenceReason', '')}\n",
    "\n",
    "Tonality score: {eval_scores.get('TonalityScore', 0):.3f}\n",
    "Tonality notes: {eval_reasons.get('TonalityReason', '')}\n",
    "\n",
    "Safety score: {eval_scores.get('SafetyScore', 0):.3f}\n",
    "Safety notes: {eval_reasons.get('SafetyReason', '')}\n",
    "\n",
    "REVISION OBJECTIVES\n",
    "1. Improve faithfulness to the document. Avoid inventing facts.\n",
    "2. Improve logical structure, flow, transitions, and clarity.\n",
    "3. Maintain exactly the requested tone: {tone}\n",
    "4. Keep the writing professional and consistent throughout.\n",
    "5. Ensure no personally identifiable information is added.\n",
    "6. Keep wording concise and remove repetition.\n",
    "\n",
    "OUTPUT FORMAT\n",
    "Return only the improved summary text. Do not explain changes.\n",
    "\"\"\"\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "\n",
    "# Gather evaluation information from Cell 4\n",
    "first_pass_scores = {\n",
    "    \"SummarizationScore\": float(evaluation_report.get(\"SummarizationScore\", 0.0)),\n",
    "    \"CoherenceScore\": float(evaluation_report.get(\"CoherenceScore\", 0.0)),\n",
    "    \"TonalityScore\": float(evaluation_report.get(\"TonalityScore\", 0.0)),\n",
    "    \"SafetyScore\": float(evaluation_report.get(\"SafetyScore\", 0.0)),\n",
    "    \"CompositeScore\": float(evaluation_report.get(\"CompositeScore\", 0.0)),\n",
    "}\n",
    "\n",
    "first_pass_reasons = {\n",
    "    \"SummarizationReason\": SummarizationReason,\n",
    "    \"CoherenceReason\": CoherenceReason,\n",
    "    \"TonalityReason\": TonalityReason,\n",
    "    \"SafetyReason\": SafetyReason,\n",
    "}\n",
    "\n",
    "# Build revision prompts and run improvement request\n",
    "system_message, user_message = _build_revision_prompt(\n",
    "    doc_text=source_text,\n",
    "    original_summary=article_summary.summary,\n",
    "    tone=article_summary.tone,\n",
    "    eval_scores=first_pass_scores,\n",
    "    eval_reasons=first_pass_reasons,\n",
    ")\n",
    "\n",
    "response_revision = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Extract and store improved summary text\n",
    "improved_summary_text = response_revision.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# Evaluate improved summary using the same metrics as before\n",
    "tc_improved = LLMTestCase(input=source_text, actual_output=improved_summary_text)\n",
    "improved_results = run_metrics(tc_improved)  # Uses function from Cell 4\n",
    "\n",
    "\n",
    "def _to_float(value):\n",
    "    \"\"\"Small helper to safely round float or score values across metric structures.\"\"\"\n",
    "    try:\n",
    "        return round(float(value), 4)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Collect revised metric scores\n",
    "improved_scores = {\n",
    "    \"SummarizationScore\": _to_float(improved_results[\"Summarization\"][0]),\n",
    "    \"CoherenceScore\": _to_float(improved_results[\"Coherence\"][0]),\n",
    "    \"TonalityScore\": _to_float(improved_results[\"Tonality\"][0]),\n",
    "    \"SafetyScore\": _to_float(improved_results[\"Safety\"][0]),\n",
    "}\n",
    "\n",
    "# Compute updated composite score with same weighting\n",
    "improved_composite = round(\n",
    "    0.5 * improved_scores[\"SummarizationScore\"]\n",
    "    + 0.2 * improved_scores[\"CoherenceScore\"]\n",
    "    + 0.2 * improved_scores[\"TonalityScore\"]\n",
    "    + 0.1 * improved_scores[\"SafetyScore\"],\n",
    "    4\n",
    ")\n",
    "\n",
    "\n",
    "# Report before/after and score deltas clearly\n",
    "comparison_report = {\n",
    "    \"Before\": first_pass_scores,\n",
    "    \"After\": {\n",
    "        \"SummarizationScore\": improved_scores[\"SummarizationScore\"],\n",
    "        \"CoherenceScore\": improved_scores[\"CoherenceScore\"],\n",
    "        \"TonalityScore\": improved_scores[\"TonalityScore\"],\n",
    "        \"SafetyScore\": improved_scores[\"SafetyScore\"],\n",
    "        \"CompositeScore\": improved_composite,\n",
    "    },\n",
    "    \"Delta\": {\n",
    "        \"SummarizationScore\": improved_scores[\"SummarizationScore\"] - first_pass_scores[\"SummarizationScore\"],\n",
    "        \"CoherenceScore\": improved_scores[\"CoherenceScore\"] - first_pass_scores[\"CoherenceScore\"],\n",
    "        \"TonalityScore\": improved_scores[\"TonalityScore\"] - first_pass_scores[\"TonalityScore\"],\n",
    "        \"SafetyScore\": improved_scores[\"SafetyScore\"] - first_pass_scores[\"SafetyScore\"],\n",
    "        \"CompositeScore\": improved_composite - first_pass_scores[\"CompositeScore\"],\n",
    "    },\n",
    "    \"ImprovedSummaryPreview\": improved_summary_text[:600] + (\n",
    "        \"...\" if len(improved_summary_text) > 600 else \"\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"===== Enhancement Results (Before vs After) =====\")\n",
    "print(json.dumps(comparison_report, indent=2))\n",
    "\n",
    "\n",
    "# Provide short written insight/interpretation for the assignment writeup\n",
    "print(\"\\n===== Interpretation =====\")\n",
    "print(\"A second model pass with evaluation-based guidance produced a revised summary.\")\n",
    "print(\"If score improvements are positive, this indicates that self-critique controlled generation improved alignment.\")\n",
    "print(\"However, this type of self-correction is limited because the evaluation signals come from another model,\")\n",
    "print(\"which may reinforce its own biases or overlook factual consistency issues.\")\n",
    "print(\"Reliable improvement usually requires source-grounded verification and additional safety guardrails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
