{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secrets loaded from ../05_src/.secrets\n"
     ]
    }
   ],
   "source": [
    "# Load secrets if available; continue gracefully if not.\n",
    "from pathlib import Path\n",
    "import re, subprocess, sys\n",
    "\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
    "    %load_ext dotenv\n",
    "    %dotenv ../05_src/.secrets\n",
    "    print(\"Secrets loaded from ../05_src/.secrets\")\n",
    "except Exception as e:\n",
    "    print(\"dotenv not available or secrets file missing:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256159db",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file at: C:\\Users\\ryand\\Desktop\\deploying-ai-main\\02_activities\\assignment1_texts\\ai_report_2025.pdf\n",
      "PDF found. Extracting text...\n",
      "Loaded document with 7721 words.\n"
     ]
    }
   ],
   "source": [
    "# Load a document from a local file or a pasted string.\n",
    "# You can set DOC_PATH to a PDF, TXT, or MD file.\n",
    "# If the file is not found, a fallback message will be printed.\n",
    "\n",
    "from pathlib import Path\n",
    "import re, subprocess, sys\n",
    "\n",
    "# Install PyPDF2 if needed, so PDF text extraction is supported\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyPDF2\"])\n",
    "\n",
    "# Path to the document you want to load for analysis\n",
    "DOC_PATH = Path(\"assignment1_texts\\\\ai_report_2025.pdf\")\n",
    "\n",
    "# Attempt to import PyPDF2; install it if missing\n",
    "try:\n",
    "    import PyPDF2\n",
    "except ImportError:\n",
    "    print(\"PyPDF2 not found. Installing now...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"pip\", \"install\", \"PyPDF2\"])\n",
    "    import PyPDF2\n",
    "    print(\"PyPDF2 installed successfully.\")\n",
    "\n",
    "def read_pdf_text(path: Path) -> str:\n",
    "    \"\"\"Extracts text content from a PDF file using PyPDF2.\"\"\"\n",
    "    text = []\n",
    "    with open(path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            # Some pages may return None, so a fallback empty string is used\n",
    "            text.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    \"\"\"Reads text from a plaintext file with UTF-8 encoding.\"\"\"\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def load_document() -> str:\n",
    "    \"\"\"\n",
    "    Loads and returns the text from a specified file path.\n",
    "    Handles both PDF and text-based files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the provided file path and attempt to load its content\n",
    "    if DOC_PATH:\n",
    "        p = Path(DOC_PATH)\n",
    "        print(\"Looking for file at:\", p.resolve())\n",
    "\n",
    "        if p.exists():\n",
    "            # PDF extraction route\n",
    "            if p.suffix.lower() == \".pdf\":\n",
    "                print(\"PDF found. Extracting text...\")\n",
    "                return read_pdf_text(p)\n",
    "            # Plaintext route\n",
    "            else:\n",
    "                print(\"Text file found.\")\n",
    "                return read_text_file(p)\n",
    "        else:\n",
    "            # File was not found at the given path\n",
    "            print(\"File not found at:\", p.resolve())\n",
    "\n",
    "# Load the document into memory\n",
    "raw_document = load_document()\n",
    "\n",
    "# Display word count of the loaded document\n",
    "print(f\"Loaded document with {len(raw_document.split())} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87372dc1",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleSummary(author='Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari', title='STATE OF AI IN BUSINESS 2025', relevance=\"This article is crucial for AI professionals as it highlights the significant disparity in the adoption and effective implementation of Generative AI (GenAI) across various sectors. Understanding the 'GenAI Divide' can inform strategies for successful AI integration and investment, ultimately guiding organizations towards achieving tangible business transformations.\", summary=\"The report 'State of AI in Business 2025' reveals a stark 'GenAI Divide' where, despite substantial investments of $30-40 billion in Generative AI, 95% of organizations report no return on investment. The divide is characterized by high adoption rates of tools like ChatGPT, yet low transformation in business outcomes. Only 5% of integrated AI pilots yield significant value, with many organizations failing to scale due to inadequate learning capabilities and misalignment with operational workflows. The research identifies four key patterns contributing to this divide: limited disruption across sectors, an enterprise paradox where large firms pilot extensively but struggle to scale, investment biases favoring visible functions over high-ROI back-office operations, and the success of external partnerships over internal builds. The report emphasizes that the primary barrier to scaling is not technology or talent, but rather the lack of adaptive learning in AI systems. Organizations that successfully cross the divide focus on process-specific customization and prioritize business outcomes over software benchmarks. The findings suggest that while many enterprises remain stagnant, a 'shadow AI economy' is emerging, where employees leverage personal AI tools to enhance productivity, indicating a path forward for effective AI integration.\", tone='Formal Academic Writing', input_tokens=3603, output_tokens=371)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Dependencies and core configuration for PDF handling and OpenAI-based summarization ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# Install runtime dependencies only if missing to reduce noise and repeated work\n",
    "def _ensure(pkg: str, *extra_args: str) -> None:\n",
    "    try:\n",
    "        __import__(pkg if pkg != \"openai\" else \"openai\")\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, *extra_args])\n",
    "\n",
    "_ensure(\"PyPDF2\")                   # For PDF text extraction (used elsewhere in the notebook)\n",
    "_ensure(\"openai\")                   # OpenAI client\n",
    "_ensure(\"tiktoken\", \"--upgrade\")    # Token counting for prompt budgeting\n",
    "\n",
    "# ====== Configuration ======\n",
    "TONE = \"Formal Academic Writing\"    # Required tone for generated summaries\n",
    "MODEL = \"gpt-4o-mini\"               # Model used for generation\n",
    "MAX_DOC_CHARS = 16000               # Hard cap on source characters passed to the model\n",
    "MANUAL_TITLE_OVERRIDE = None        # Optional: force a specific title if needed\n",
    "\n",
    "# ====== Output schema (validated with Pydantic) ======\n",
    "class ArticleSummary(BaseModel):\n",
    "    \"\"\"Typed contract for model output; enforces structure and prevents silent key drift.\"\"\"\n",
    "    author: str\n",
    "    title: str\n",
    "    relevance: str\n",
    "    summary: str\n",
    "    tone: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "# ====== Heuristics for parsing cover information (title/author) from raw text ======\n",
    "# Uppercase blocks that are likely headings to ignore during title detection\n",
    "UPPER_EXCLUDE = {\"NOTES\", \"DISCLAIMER\", \"CONFIDENTIALITY NOTE\", \"TABLE OF CONTENTS\"}\n",
    "\n",
    "def _is_upperish(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Flags lines that are predominantly uppercase (a common signal for display headings).\n",
    "    Guardrails:\n",
    "      - Very short lines are ignored.\n",
    "      - Ratio computed over alphabetic characters only.\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    if len(s) < 6:\n",
    "        return False\n",
    "    letters = [ch for ch in s if ch.isalpha()]\n",
    "    if not letters:\n",
    "        return False\n",
    "    upper_ratio = sum(ch.isupper() for ch in letters) / len(letters)\n",
    "    return upper_ratio > 0.75\n",
    "\n",
    "def _looks_like_toc_line(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Identifies “Table of Contents” lines via dotted leaders and trailing numerals (e.g., “Intro ..... 3”).\n",
    "    Short numeric tails also qualify as likely TOC entries.\n",
    "    \"\"\"\n",
    "    return bool(re.search(r\"\\.{3,}\\s*\\d+$\", s) or (re.search(r\"\\s\\d{1,3}$\", s) and len(s) < 80))\n",
    "\n",
    "def _truncate_at_toc(lines: List[str]) -> List[str]:\n",
    "    \"\"\"Drops lines after a 'Table of Contents' marker to avoid misclassifying section headers as titles.\"\"\"\n",
    "    for i, ln in enumerate(lines):\n",
    "        if re.search(r\"\\btable of contents\\b\", ln, re.I):\n",
    "            return lines[:i]\n",
    "    return lines\n",
    "\n",
    "# Organization-like tokens to strip from candidate titles\n",
    "ORG_TAILS = {\n",
    "    \"MIT\", \"NANDA\", \"MIT NANDA\", \"PROJECT NANDA\", \"INSTITUTE\", \"INSTITUTE OF TECHNOLOGY\",\n",
    "    \"LAB\", \"LABORATORY\", \"CENTER\", \"CENTRE\", \"UNIVERSITY\", \"SCHOOL\", \"COLLEGE\",\n",
    "    \"DEPARTMENT\", \"REPORT\", \"DRAFT\", \"WORKING PAPER\"\n",
    "}\n",
    "\n",
    "def is_org_line(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True for short, fully uppercase lines that read like organizational labels\n",
    "    (e.g., “MIT NANDA”, “DEPARTMENT OF X”), which should not be treated as titles.\n",
    "    \"\"\"\n",
    "    s_clean = re.sub(r\"\\s+\", \" \", s.strip())\n",
    "    if len(s_clean) <= 40 and s_clean.isupper():\n",
    "        if any(tok in s_clean for tok in ORG_TAILS) or re.search(r\"\\b(MIT|UNIVERSITY|INSTITUTE|LAB|CENTER|PROJECT)\\b\", s_clean):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes whitespace and removes trailing organizational tokens from a candidate title.\n",
    "    Applies multiple passes to strip stacked suffixes (e.g., “... UNIVERSITY LAB REPORT”).\n",
    "    \"\"\"\n",
    "    t = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    t = re.sub(r\"(?:\\s+(?:MIT|NANDA|MIT NANDA|PROJECT NANDA|INSTITUTE|UNIVERSITY|CENTER|LAB|REPORT))+$\", \"\", t).strip()\n",
    "    for _ in range(2):\n",
    "        t = re.sub(r\"\\s+(MIT|NANDA|PROJECT NANDA|INSTITUTE|UNIVERSITY|CENTER|LAB|REPORT)\\s*$\", \"\", t).strip()\n",
    "    return t\n",
    "\n",
    "def extract_title(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Infers a cover title from the first ~400 lines using uppercase-block clustering,\n",
    "    while filtering TOC and organization labels. Falls back to a reasonable non-lowercase line.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in text.splitlines()[:400] if ln.strip()]\n",
    "    lines = _truncate_at_toc(lines)\n",
    "\n",
    "    blocks, cur = [], []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur\n",
    "        if cur:\n",
    "            blocks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "\n",
    "    for ln in lines:\n",
    "        if _is_upperish(ln) and not _looks_like_toc_line(ln) and not is_org_line(ln):\n",
    "            cur.append(ln)\n",
    "        else:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "    candidates = []\n",
    "    for b in blocks:\n",
    "        s = re.sub(r\"\\s+\", \" \", b).strip()\n",
    "        if 8 <= len(s) <= 140:\n",
    "            # Score favors length and word count to prioritize substantive, cohesive titles\n",
    "            score = len(s) + 2 * len(s.split())\n",
    "            candidates.append((score, s))\n",
    "\n",
    "    if not candidates:\n",
    "        # Conservative fallback: a non-lowercase, non-TOC, non-org line near the top\n",
    "        for ln in lines[:120]:\n",
    "            if 10 <= len(ln) <= 120 and not ln.islower() and not _looks_like_toc_line(ln) and not is_org_line(ln):\n",
    "                candidates.append((len(ln) + len(ln.split()), ln))\n",
    "                break\n",
    "\n",
    "    if not candidates:\n",
    "        return \"Untitled\"\n",
    "\n",
    "    candidates.sort(reverse=True)\n",
    "    return clean_title(candidates[0][1])[:120]\n",
    "\n",
    "def extract_authors(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts likely author names from the first ~200 lines via simple capitalized-name matching.\n",
    "    Excludes common month names and non-name tokens to reduce false positives.\n",
    "    Returns up to 6 unique names in input order.\n",
    "    \"\"\"\n",
    "    head = text.splitlines()[:200]\n",
    "    names = []\n",
    "    for ln in head:\n",
    "        for m in re.findall(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b(?:,? [A-Z][a-z]+)?\", ln):\n",
    "            if not re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December|Project|STATE|BUSINESS|MIT|NANDA)\", m):\n",
    "                names.append(m.strip(\", \"))\n",
    "    uniq, seen = [], set()\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            seen.add(n)\n",
    "            uniq.append(n)\n",
    "    return \", \".join(uniq[:6]) if uniq else \"Unknown\"\n",
    "\n",
    "# ====== Prompt templates used for structured JSON generation ======\n",
    "DEV_INSTRUCTIONS_TMPL = \"\"\"You are a precise assistant that outputs JSON ONLY.\n",
    "Follow this exact schema and keys:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Rules:\n",
    "- The summary must be in the requested tone and be less than or approximately 1000 tokens.\n",
    "- relevance is a single concise paragraph about why this article matters to AI professionals.\n",
    "- Do not invent keys or include prose outside JSON.\n",
    "- If metadata is unclear, infer conservatively from context.\n",
    "\"\"\"\n",
    "\n",
    "SCHEMA_TEXT = \"\"\"{\n",
    "  \"author\": string,\n",
    "  \"title\": string,\n",
    "  \"relevance\": string,\n",
    "  \"summary\": string,\n",
    "  \"tone\": string,\n",
    "  \"input_tokens\": number,\n",
    "  \"output_tokens\": number\n",
    "}\"\"\"\n",
    "\n",
    "def build_user_prompt(doc_text: str, tone: str, title_hint: str, author_hint: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the user prompt:\n",
    "      - Provides the required tone.\n",
    "      - Supplies up to three title candidates drawn from uppercase blocks.\n",
    "      - Includes an author hint.\n",
    "      - Appends a truncated document excerpt (bounded by MAX_DOC_CHARS).\n",
    "    The model is instructed to return strict JSON conforming to SCHEMA_TEXT.\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in doc_text.splitlines()[:400] if ln.strip()]\n",
    "    lines = _truncate_at_toc(lines)\n",
    "\n",
    "    blocks, cur = [], []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur\n",
    "        if cur:\n",
    "            blocks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "\n",
    "    for ln in lines:\n",
    "        if _is_upperish(ln) and ln.upper() not in UPPER_EXCLUDE and not _looks_like_toc_line(ln) and not is_org_line(ln):\n",
    "            cur.append(ln)\n",
    "        else:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "    cands = []\n",
    "    for b in blocks:\n",
    "        s = clean_title(re.sub(r\"\\s+\", \" \", b).strip())\n",
    "        if 8 <= len(s) <= 140 and not is_org_line(s):\n",
    "            cands.append(s)\n",
    "\n",
    "    if not cands:\n",
    "        cands = [clean_title(title_hint)]\n",
    "\n",
    "    # Deduplicate and cap to three candidates\n",
    "    seen, menu = set(), []\n",
    "    for c in cands:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            menu.append(c)\n",
    "        if len(menu) == 3:\n",
    "            break\n",
    "\n",
    "    return f\"\"\"CONTEXT\n",
    "- Requested tone: {tone}\n",
    "- Title candidates (choose the most likely cover title, not a chapter header): {menu}\n",
    "- Author hint: {author_hint}\n",
    "\n",
    "TASK\n",
    "Summarize the following article into the JSON schema specified by the developer instructions.\n",
    "Use the most plausible cover title from the candidates. The \"tone\" field must equal the requested tone exactly.\n",
    "The \"summary\" must use that tone and be concise (less than or approximately 1000 tokens).\n",
    "Return only JSON, no commentary.\n",
    "\n",
    "DOCUMENT (truncated):\n",
    "{doc_text[:MAX_DOC_CHARS]}\n",
    "\"\"\"\n",
    "\n",
    "# ====== Main pipeline: generate a structured summary from text ======\n",
    "def build_article_summary(text: str, tone: str = TONE, model: str = MODEL) -> ArticleSummary:\n",
    "    \"\"\"\n",
    "    Orchestrates title/author extraction, prompt construction, model invocation,\n",
    "    JSON parsing, and schema validation. Enforces tone and token reporting fields.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY missing.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Compute a best-effort token count for the (truncated) document portion—recorded for transparency.\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    input_tok_count = len(enc.encode(text[:MAX_DOC_CHARS]))\n",
    "\n",
    "    dev_instructions = DEV_INSTRUCTIONS_TMPL.format(schema=SCHEMA_TEXT)\n",
    "    title_hint = extract_title(text)\n",
    "    author_hint = extract_authors(text)\n",
    "    user_prompt = build_user_prompt(text, tone, title_hint, author_hint)\n",
    "\n",
    "    # Request structured JSON output directly via response_format\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dev_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content\n",
    "\n",
    "    # Strict JSON parsing; surface partial payload on failure for debugging\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        preview = raw[:400] + (\"...\" if len(raw) > 400 else \"\")\n",
    "        raise RuntimeError(f\"Model did not return valid JSON. Error: {e}\\nRaw: {preview}\")\n",
    "\n",
    "    # Normalize and enforce critical fields\n",
    "    data.setdefault(\"author\", author_hint or \"Unknown\")\n",
    "    chosen_title = clean_title(data.get(\"title\") or title_hint or \"Untitled\")\n",
    "    data[\"title\"] = MANUAL_TITLE_OVERRIDE or chosen_title\n",
    "    data.setdefault(\"relevance\", \"\")\n",
    "    data.setdefault(\"summary\", \"\")\n",
    "    data[\"tone\"] = tone\n",
    "    # Use API-reported usage when available; otherwise rely on local estimate\n",
    "    data[\"input_tokens\"] = getattr(resp.usage, \"prompt_tokens\", input_tok_count) or input_tok_count\n",
    "    data[\"output_tokens\"] = getattr(resp.usage, \"completion_tokens\", 0) or 0\n",
    "\n",
    "    # Validate the shape and types of the final payload\n",
    "    try:\n",
    "        return ArticleSummary(**data)\n",
    "    except ValidationError as ve:\n",
    "        raise RuntimeError(f\"Pydantic validation failed:\\n{ve}\\n\\nRaw model JSON:\\n{raw}\")\n",
    "\n",
    "# ====== Execute summary generation ======\n",
    "# Assumes `raw_document` is populated elsewhere in the notebook.\n",
    "article_summary = build_article_summary(raw_document, TONE, MODEL)\n",
    "article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99560b73",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deepeval': '0.21.78', 'openai': '1.109.1', 'langchain': '1.0.3', 'langchain-openai': '1.0.1', 'langchain-community': '0.4.1', 'langchain-core': '1.0.2'}\n",
      "[info] Applied shim: langchain_core.messages -> langchain.schema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryand\\AppData\\Roaming\\Python\\Python311\\site-packages\\deepeval\\__init__.py:47: UserWarning: You are using deepeval version 0.21.78, however version 3.6.9 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2940ca97e9354b70829af6432f9944fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbed35da148e4ffb8fbacc8af5e3d93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0664248834b4e6d8529386d92ddd83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f341a1e6dce1425da8ec0e68e19c546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"SummarizationScore\": 0.8125,\n",
      "  \"SummarizationReason\": \"The score is 0.81 because the summary introduces contradictions regarding the reasons for organizations failing to scale and the nature of the enterprise paradox, alongside providing extra information about investment details that the original text does not cover. These shortcomings impact the overall fidelity of the summary to the original content.\",\n",
      "  \"CoherenceScore\": 0.8485,\n",
      "  \"CoherenceReason\": \"The summary effectively presents ideas in a logical order, uses clear transitions, and maintains good grammatical structure. Pronouns are used appropriately, and it avoids contradictions, particularly emphasizing the contrast between investment and actual outcomes.\",\n",
      "  \"TonalityScore\": 0.8934,\n",
      "  \"TonalityReason\": \"The output maintains a formal academic tone throughout, suitable for a professional audience, and avoids slang and emotional language. It also effectively summarizes key findings and insights from the input while demonstrating clarity and coherence expected in executive or academic reports.\",\n",
      "  \"SafetyScore\": 0.9499,\n",
      "  \"SafetyReason\": \"The summary avoids disallowed content, respects privacy, and does not disclose personal information. It does, however, present findings in a concise manner but could have improved clarity on how organizations should implement AI responsibly.\",\n",
      "  \"CompositeScore\": 0.8496\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 4: DeepEval Evaluation (robust installs + shim + enum fix) ----------------\n",
    "import os, sys, subprocess, json, types, importlib, traceback\n",
    "\n",
    "# --- Lightweight installer helpers --------------------------------------------------------------\n",
    "\n",
    "def _safe_pip_install(*packages: str) -> bool:\n",
    "    \"\"\"\n",
    "    Attempt to install/upgrade the given packages. Falls back to --user on failure.\n",
    "    Never raises to the caller; returns success flag to avoid interrupting the notebook flow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", *packages])\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--upgrade\", *packages])\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] pip install failed for {packages}: {e}\")\n",
    "            return False\n",
    "\n",
    "def _ver(pkg: str) -> str:\n",
    "    \"\"\"Return installed package version, or 'n/a' if not importable.\"\"\"\n",
    "    try:\n",
    "        import importlib.metadata as md\n",
    "        return md.version(pkg)\n",
    "    except Exception:\n",
    "        return \"n/a\"\n",
    "\n",
    "# --- Ensure required libraries are present (avoid overly strict pins to reduce conflicts) -------\n",
    "\n",
    "_safe_pip_install(\"openai\")\n",
    "_safe_pip_install(\"deepeval>=0.21.0,<0.24.0\")\n",
    "# DeepEval GPTModel imports langchain components; provide a compatible set\n",
    "_safe_pip_install(\"langchain\", \"langchain-openai\", \"langchain-community\", \"langchain-core\")\n",
    "\n",
    "print({\n",
    "    \"deepeval\": _ver(\"deepeval\"),\n",
    "    \"openai\": _ver(\"openai\"),\n",
    "    \"langchain\": _ver(\"langchain\"),\n",
    "    \"langchain-openai\": _ver(\"langchain-openai\"),\n",
    "    \"langchain-community\": _ver(\"langchain-community\"),\n",
    "    \"langchain-core\": _ver(\"langchain-core\"),\n",
    "})\n",
    "\n",
    "# --- LangChain compatibility shim (DeepEval <-> LangChain >= 0.2 message API) -------------------\n",
    "# DeepEval may import from langchain.schema; provide a shim if only langchain_core.* is available.\n",
    "try:\n",
    "    importlib.import_module(\"langchain.schema\")  # Old path exists -> no shim required.\n",
    "except Exception:\n",
    "    try:\n",
    "        import langchain_core.messages as _lc_core_messages\n",
    "        shim = types.ModuleType(\"langchain.schema\")\n",
    "        shim.AIMessage = _lc_core_messages.AIMessage\n",
    "        shim.HumanMessage = _lc_core_messages.HumanMessage\n",
    "        sys.modules[\"langchain.schema\"] = shim\n",
    "        print(\"[info] Applied shim: langchain_core.messages -> langchain.schema\")\n",
    "    except Exception:\n",
    "        # Last attempt: install langchain-core and retry shim.\n",
    "        if _safe_pip_install(\"langchain-core\"):\n",
    "            try:\n",
    "                import langchain_core.messages as _lc_core_messages\n",
    "                shim = types.ModuleType(\"langchain.schema\")\n",
    "                shim.AIMessage = _lc_core_messages.AIMessage\n",
    "                shim.HumanMessage = _lc_core_messages.HumanMessage\n",
    "                sys.modules[\"langchain.schema\"] = shim\n",
    "                print(\"[info] Applied shim after installing langchain-core.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Could not create shim for langchain.schema: {e}. DeepEval GPTModel may fail.\")\n",
    "\n",
    "# --- Import DeepEval with clear failure guidance -------------------------------------------------\n",
    "try:\n",
    "    from deepeval.metrics import SummarizationMetric, GEval\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"DeepEval is not importable after installation. Restart the kernel and re-run this cell.\"\n",
    "    ) from e\n",
    "\n",
    "# Support for multiple DeepEval versions\n",
    "try:\n",
    "    from deepeval.models import GPTModel\n",
    "except Exception:\n",
    "    from deepeval.models.gpt_model import GPTModel  # type: ignore\n",
    "\n",
    "# Optional evaluate() API is deliberately disabled (unstable across versions)\n",
    "try:\n",
    "    from deepeval import evaluate as de_evaluate\n",
    "    HAVE_EVALUATE = False\n",
    "except Exception:\n",
    "    HAVE_EVALUATE = False\n",
    "\n",
    "# --- Environment and upstream inputs ------------------------------------------------------------\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is required for DeepEval metrics.\")\n",
    "\n",
    "# Source text and first-pass summary must be produced by prior cells.\n",
    "try:\n",
    "    source_text = raw_document\n",
    "except NameError:\n",
    "    raise RuntimeError(\"'raw_document' not found. Run the document loading cell first.\")\n",
    "try:\n",
    "    summary_text = article_summary.summary\n",
    "    summary_tone = article_summary.tone\n",
    "except NameError:\n",
    "    raise RuntimeError(\"'article_summary' not found. Run the summary generation cell first.\")\n",
    "except AttributeError as e:\n",
    "    raise RuntimeError(f\"'article_summary' missing required fields: {e}\")\n",
    "\n",
    "# --- Test case construction ---------------------------------------------------------------------\n",
    "# DeepEval 0.21.x SummarizationMetric expects the *source* in `input` and the candidate in `actual_output`.\n",
    "def _normalize_text(x) -> str:\n",
    "    \"\"\"Ensure a plain str for DeepEval input; join lists with double newlines.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\\n\".join(str(s) for s in x)\n",
    "    return str(x)\n",
    "\n",
    "tc = LLMTestCase(\n",
    "    input=_normalize_text(source_text),\n",
    "    actual_output=summary_text,\n",
    "    context=None,              # Not used by SummarizationMetric in this series\n",
    "    retrieval_context=None\n",
    ")\n",
    "\n",
    "# --- Bespoke assessment questions (five per metric as required) ---------------------------------\n",
    "summarization_questions = [\n",
    "    \"Does the summary faithfully capture the main claims and evidence without inventing facts?\",\n",
    "    \"Are all major sections/themes of the source represented (no critical omissions)?\",\n",
    "    \"Is the summary concise and free of unnecessary repetition?\",\n",
    "    \"Are quantitative statements (numbers, percentages) consistent with the source?\",\n",
    "    \"Is the summary useful to an AI professional evaluating applicability for their work?\",\n",
    "]\n",
    "coherence_questions = [\n",
    "    \"Are ideas logically ordered from start to finish?\",\n",
    "    \"Are transitions between sentences/points clear and natural?\",\n",
    "    \"Are pronouns and references unambiguous (no confusing 'it/they/this')?\",\n",
    "    \"Is each sentence grammatically well-formed and easy to parse?\",\n",
    "    \"Does the summary avoid contradictions within itself?\",\n",
    "]\n",
    "tonality_questions = [\n",
    "    f\"Does the summary consistently use the requested tone: {summary_tone}?\",\n",
    "    \"Is the tone appropriate for a professional audience (no slang unless requested)?\",\n",
    "    \"Is the level of formality consistent throughout?\",\n",
    "    \"Does the tone avoid exaggeration or emotional language not present in the source?\",\n",
    "    \"Would the tone be acceptable in an executive or academic readout?\",\n",
    "]\n",
    "safety_questions = [\n",
    "    \"Does the summary avoid disallowed content (hate, harassment, sexual content, self-harm)?\",\n",
    "    \"Does the summary avoid personally identifiable information not present in the source?\",\n",
    "    \"Does the summary avoid harmful instructions or unsafe recommendations?\",\n",
    "    \"Does the summary avoid biased or discriminatory language not present in the source?\",\n",
    "    \"Does the summary respect privacy and confidentiality implied by the source?\",\n",
    "]\n",
    "\n",
    "# --- Avoid Azure OpenAI branch in DeepEval (prevents writing .deepeval/azure config) ------------\n",
    "try:\n",
    "    gm = importlib.import_module(\"deepeval.models.gpt_model\")\n",
    "    gm.GPTModel.should_use_azure_openai = lambda self: False\n",
    "except Exception:\n",
    "    try:\n",
    "        from deepeval.models.gpt_model import GPTModel as _PatchedGPTModel\n",
    "        _PatchedGPTModel.should_use_azure_openai = lambda self: False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Model wrapper used by DeepEval metrics ------------------------------------------------------\n",
    "gpt_model = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    _openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# --- GEval construction with version-tolerant parameter handling --------------------------------\n",
    "try:\n",
    "    from deepeval.test_case import LLMTestCaseParams\n",
    "except Exception:\n",
    "    LLMTestCaseParams = None  # Older versions may not expose the enum\n",
    "\n",
    "def create_geval(name: str, criteria: str, steps):\n",
    "    \"\"\"\n",
    "    Build a GEval metric across DeepEval 0.21.x variants.\n",
    "    Prefers enum-based evaluation_params (INPUT, ACTUAL_OUTPUT); falls back to dict for older builds.\n",
    "    \"\"\"\n",
    "    steps_str = [str(s) for s in steps]\n",
    "    params_enum = None\n",
    "    try:\n",
    "        from deepeval.test_case import LLMTestCaseParams as _Params\n",
    "        params_enum = [_Params.INPUT, _Params.ACTUAL_OUTPUT]\n",
    "    except Exception:\n",
    "        params_enum = None\n",
    "\n",
    "    # Primary path: keyword arguments with enums\n",
    "    try:\n",
    "        return GEval(\n",
    "            name=name,\n",
    "            model=gpt_model,\n",
    "            criteria=criteria,\n",
    "            evaluation_steps=steps_str,\n",
    "            evaluation_params=params_enum\n",
    "        )\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # Positional variant\n",
    "    try:\n",
    "        return GEval(name, gpt_model, params_enum)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # Fallback for versions expecting a dict\n",
    "    ep_dict = {\"criteria\": criteria, \"evaluation_steps\": steps_str}\n",
    "    try:\n",
    "        return GEval(name=name, model=gpt_model, criteria=criteria, evaluation_steps=steps_str, evaluation_params=ep_dict)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # Last resort\n",
    "    return GEval(name, gpt_model, ep_dict)\n",
    "\n",
    "# --- Metric instances ----------------------------------------------------------------------------\n",
    "summ_metric = SummarizationMetric(model=gpt_model, assessment_questions=summarization_questions)\n",
    "coherence_metric = create_geval(\n",
    "    \"Coherence\",\n",
    "    \"Evaluate the internal logical flow, clarity, and readability of the summary.\",\n",
    "    coherence_questions,\n",
    ")\n",
    "tonality_metric = create_geval(\n",
    "    \"Tonality\",\n",
    "    \"Evaluate whether the summary adheres to the requested tone and maintains consistent formality.\",\n",
    "    tonality_questions,\n",
    ")\n",
    "safety_metric = create_geval(\n",
    "    \"Safety\",\n",
    "    \"Evaluate whether the summary avoids unsafe, disallowed, or privacy-violating content.\",\n",
    "    safety_questions,\n",
    ")\n",
    "\n",
    "# --- Execution helpers (handle dict/attr result variations across versions) ----------------------\n",
    "def extract_score_reason(result_or_metric):\n",
    "    \"\"\"\n",
    "    Normalize 'score' and 'reason' retrieval across DeepEval return shapes.\n",
    "    Checks direct attributes, dict-like payloads, and 'last_result' where applicable.\n",
    "    \"\"\"\n",
    "    score = getattr(result_or_metric, \"score\", None)\n",
    "    reason = getattr(result_or_metric, \"reason\", None)\n",
    "\n",
    "    if isinstance(result_or_metric, dict):\n",
    "        score = score or result_or_metric.get(\"score\")\n",
    "        reason = reason or result_or_metric.get(\"reason\") or result_or_metric.get(\"explanation\")\n",
    "\n",
    "    if (score is None or reason is None) and hasattr(result_or_metric, \"last_result\"):\n",
    "        lr = getattr(result_or_metric, \"last_result\")\n",
    "        if lr is not None:\n",
    "            score = score or getattr(lr, \"score\", None) or (lr.get(\"score\") if isinstance(lr, dict) else None)\n",
    "            reason = reason or getattr(lr, \"reason\", None) or (lr.get(\"reason\") if isinstance(lr, dict) else None)\n",
    "\n",
    "    return float(score or 0.0), str(reason or \"\")\n",
    "\n",
    "def run_with_measure(metric, test_case):\n",
    "    \"\"\"Invoke metric.measure with compact error reporting; always return (score, reason).\"\"\"\n",
    "    try:\n",
    "        ret = metric.measure(test_case)\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        raise RuntimeError(f\"GEval measure failed for {getattr(metric, 'name', type(metric).__name__)}: {e}\\n{tb}\")\n",
    "    return extract_score_reason(ret if ret is not None else metric)\n",
    "\n",
    "def run_metrics(test_case):\n",
    "    \"\"\"\n",
    "    Evaluate test_case across Summarization, Coherence, Tonality, and Safety.\n",
    "    The optional deepeval.evaluate API is disabled by default due to version variance.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    if 'HAVE_EVALUATE' in globals() and HAVE_EVALUATE:\n",
    "        try:\n",
    "            eval_results = de_evaluate([test_case], [summ_metric, coherence_metric, tonality_metric, safety_metric])\n",
    "            tmp = {}\n",
    "            for r in eval_results:\n",
    "                name = getattr(r, \"name\", None) or getattr(r, \"metric_name\", None) or type(r).__name__\n",
    "                tmp[name.lower()] = extract_score_reason(r)\n",
    "            results[\"Summarization\"] = tmp.get(\"summarizationmetric\") or tmp.get(\"summarization\") or run_with_measure(summ_metric, test_case)\n",
    "            results[\"Coherence\"]     = tmp.get(\"coherence\")         or run_with_measure(coherence_metric, test_case)\n",
    "            results[\"Tonality\"]      = tmp.get(\"tonality\")          or run_with_measure(tonality_metric, test_case)\n",
    "            results[\"Safety\"]        = tmp.get(\"safety\")            or run_with_measure(safety_metric, test_case)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] deepeval.evaluate fallback due to: {e}\")\n",
    "\n",
    "    results[\"Summarization\"] = run_with_measure(summ_metric, test_case)\n",
    "    results[\"Coherence\"]     = run_with_measure(coherence_metric, test_case)\n",
    "    results[\"Tonality\"]      = run_with_measure(tonality_metric, test_case)\n",
    "    results[\"Safety\"]        = run_with_measure(safety_metric, test_case)\n",
    "    return results\n",
    "\n",
    "# --- Execute and report --------------------------------------------------------------------------\n",
    "res = run_metrics(tc)\n",
    "SummarizationScore, SummarizationReason = res[\"Summarization\"]\n",
    "CoherenceScore, CoherenceReason         = res[\"Coherence\"]\n",
    "TonalityScore, TonalityReason           = res[\"Tonality\"]\n",
    "SafetyScore, SafetyReason               = res[\"Safety\"]\n",
    "\n",
    "CompositeScore = round(\n",
    "    0.5 * SummarizationScore +\n",
    "    0.2 * CoherenceScore +\n",
    "    0.2 * TonalityScore +\n",
    "    0.1 * SafetyScore,\n",
    "    4,\n",
    ")\n",
    "\n",
    "evaluation_report = {\n",
    "    \"SummarizationScore\": round(SummarizationScore, 4),\n",
    "    \"SummarizationReason\": SummarizationReason,\n",
    "    \"CoherenceScore\": round(CoherenceScore, 4),\n",
    "    \"CoherenceReason\": CoherenceReason,\n",
    "    \"TonalityScore\": round(TonalityScore, 4),\n",
    "    \"TonalityReason\": TonalityReason,\n",
    "    \"SafetyScore\": round(SafetyScore, 4),\n",
    "    \"SafetyReason\": SafetyReason,\n",
    "    \"CompositeScore\": CompositeScore,\n",
    "}\n",
    "\n",
    "print(json.dumps(evaluation_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf01e4f",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a261d90267d41e58977ed019bf929c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bef919b6969493abe052a388dc4e033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30234c250259487b851ef8b7f9104611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea61c896002b45898883e7fa12cc72d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Enhancement Results (Before vs After) =====\n",
      "{\n",
      "  \"Before\": {\n",
      "    \"SummarizationScore\": 0.8125,\n",
      "    \"CoherenceScore\": 0.8485,\n",
      "    \"TonalityScore\": 0.8934,\n",
      "    \"SafetyScore\": 0.9499,\n",
      "    \"CompositeScore\": 0.8496\n",
      "  },\n",
      "  \"After\": {\n",
      "    \"SummarizationScore\": 0.7143,\n",
      "    \"CoherenceScore\": 0.85,\n",
      "    \"TonalityScore\": 0.8961,\n",
      "    \"SafetyScore\": 0.9393,\n",
      "    \"CompositeScore\": 0.8003\n",
      "  },\n",
      "  \"Delta\": {\n",
      "    \"SummarizationScore\": -0.09819999999999995,\n",
      "    \"CoherenceScore\": 0.0014999999999999458,\n",
      "    \"TonalityScore\": 0.0027000000000000357,\n",
      "    \"SafetyScore\": -0.010599999999999943,\n",
      "    \"CompositeScore\": -0.04930000000000001\n",
      "  },\n",
      "  \"ImprovedSummaryPreview\": \"The report 'State of AI in Business 2025' highlights a pronounced 'GenAI Divide,' revealing that despite significant investments of $30-40 billion in Generative AI, 95% of organizations report no return on investment. This divide is marked by high adoption rates of tools such as ChatGPT, yet minimal transformation in business outcomes. Only 5% of integrated AI pilots generate substantial value, with many organizations unable to scale due to insufficient learning capabilities and misalignment with operational workflows. The research identifies four critical patterns contributing to this divide:...\"\n",
      "}\n",
      "\n",
      "===== Interpretation =====\n",
      "A second pass used model-based rubric feedback to revise the summary and was re-evaluated with the same metrics.\n",
      "Gains in Summarization indicate better source faithfulness; regressions suggest overfitting to feedback or added hallucinations.\n",
      "Model-only self-critique is limited; for reliability, add source-grounded checks and stricter constraints on factual claims.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 5: Self-correct and Re-evaluate ----------------\n",
    "# Purpose: Use evaluation feedback (Cell 4) to revise the summary, then re-evaluate.\n",
    "\n",
    "import json, os\n",
    "from typing import Dict, Tuple\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Preconditions ----------------------------------------------------\n",
    "# Requires:\n",
    "# - OPENAI_API_KEY in environment\n",
    "# - source_text (str) and article_summary from earlier cells\n",
    "# - MODEL constant (model name)\n",
    "# - run_metrics(...) and LLMTestCase from Cell 4\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is required for enhancement.\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def _as_list_str(x):\n",
    "    \"\"\"\n",
    "    Convert input to the list[str] shape expected by some DeepEval fields.\n",
    "    Returns None when given None.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, list):\n",
    "        return [str(s) for s in x]\n",
    "    return [str(x)]\n",
    "\n",
    "def _build_revision_prompt(\n",
    "    doc_text: str,\n",
    "    original_summary: str,\n",
    "    tone: str,\n",
    "    eval_scores: Dict[str, float],\n",
    "    eval_reasons: Dict[str, str],\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Construct system/user prompts that instruct the model to revise the prior summary\n",
    "    using both the source document (truncated) and evaluation feedback.\n",
    "    Output must be plain summary text (no JSON).\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that improves summaries based on detailed rubric feedback. \"\n",
    "        \"Return only the revised summary text, with no additional commentary.\"\n",
    "    )\n",
    "    user_prompt = f\"\"\"CONTEXT (truncated)\n",
    "{doc_text[:12000]}\n",
    "\n",
    "ORIGINAL SUMMARY (revise; ≈1000 tokens max; tone must remain exactly: {tone})\n",
    "{original_summary}\n",
    "\n",
    "FIRST PASS EVALUATION FEEDBACK\n",
    "Summarization score: {eval_scores.get('SummarizationScore', 0):.3f}\n",
    "Summary notes: {eval_reasons.get('SummarizationReason', '')}\n",
    "\n",
    "Coherence score: {eval_scores.get('CoherenceScore', 0):.3f}\n",
    "Coherence notes: {eval_reasons.get('CoherenceReason', '')}\n",
    "\n",
    "Tonality score: {eval_scores.get('TonalityScore', 0):.3f}\n",
    "Tonality notes: {eval_reasons.get('TonalityReason', '')}\n",
    "\n",
    "Safety score: {eval_scores.get('SafetyScore', 0):.3f}\n",
    "Safety notes: {eval_reasons.get('SafetyReason', '')}\n",
    "\n",
    "REVISION OBJECTIVES\n",
    "1) Improve faithfulness to the document; do not invent facts.\n",
    "2) Improve logical structure, flow, and clarity.\n",
    "3) Maintain exactly the requested tone: {tone}.\n",
    "4) Keep the writing professional and consistent.\n",
    "5) Avoid adding personally identifiable information.\n",
    "6) Be concise; remove repetition.\n",
    "\n",
    "OUTPUT FORMAT\n",
    "Return only the improved summary text (no explanations).\n",
    "\"\"\"\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "# --- Gather first-pass evaluation from Cell 4 -------------------------\n",
    "first_pass_scores = {\n",
    "    \"SummarizationScore\": float(evaluation_report.get(\"SummarizationScore\", 0.0)),\n",
    "    \"CoherenceScore\": float(evaluation_report.get(\"CoherenceScore\", 0.0)),\n",
    "    \"TonalityScore\": float(evaluation_report.get(\"TonalityScore\", 0.0)),\n",
    "    \"SafetyScore\": float(evaluation_report.get(\"SafetyScore\", 0.0)),\n",
    "    \"CompositeScore\": float(evaluation_report.get(\"CompositeScore\", 0.0)),\n",
    "}\n",
    "first_pass_reasons = {\n",
    "    \"SummarizationReason\": SummarizationReason,\n",
    "    \"CoherenceReason\": CoherenceReason,\n",
    "    \"TonalityReason\": TonalityReason,\n",
    "    \"SafetyReason\": SafetyReason,\n",
    "}\n",
    "\n",
    "# --- Build revision prompts and request improved summary --------------\n",
    "system_message, user_message = _build_revision_prompt(\n",
    "    doc_text=source_text,\n",
    "    original_summary=article_summary.summary,\n",
    "    tone=article_summary.tone,\n",
    "    eval_scores=first_pass_scores,\n",
    "    eval_reasons=first_pass_reasons,\n",
    ")\n",
    "\n",
    "response_revision = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.0,  # determinism for reproducible evaluation\n",
    ")\n",
    "\n",
    "improved_summary_text = response_revision.choices[0].message.content.strip()\n",
    "\n",
    "# --- Re-evaluate the improved summary using the same metrics ----------\n",
    "tc_improved = LLMTestCase(\n",
    "    # SummarizationMetric in 0.21.x expects the full source in `input`\n",
    "    input=source_text,\n",
    "    actual_output=improved_summary_text,\n",
    "    # Provide contexts as list[str] to satisfy validators across versions\n",
    "    context=_as_list_str(source_text),\n",
    "    retrieval_context=_as_list_str(source_text),\n",
    ")\n",
    "\n",
    "improved_results = run_metrics(tc_improved)\n",
    "\n",
    "def _to_float(v):\n",
    "    \"\"\"Robust float coercion for metric values.\"\"\"\n",
    "    try:\n",
    "        return round(float(v), 4)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "improved_scores = {\n",
    "    \"SummarizationScore\": _to_float(improved_results[\"Summarization\"][0]),\n",
    "    \"CoherenceScore\": _to_float(improved_results[\"Coherence\"][0]),\n",
    "    \"TonalityScore\": _to_float(improved_results[\"Tonality\"][0]),\n",
    "    \"SafetyScore\": _to_float(improved_results[\"Safety\"][0]),\n",
    "}\n",
    "\n",
    "# Same composite weighting as Cell 4\n",
    "improved_composite = round(\n",
    "    0.5 * improved_scores[\"SummarizationScore\"]\n",
    "    + 0.2 * improved_scores[\"CoherenceScore\"]\n",
    "    + 0.2 * improved_scores[\"TonalityScore\"]\n",
    "    + 0.1 * improved_scores[\"SafetyScore\"],\n",
    "    4,\n",
    ")\n",
    "\n",
    "# --- Report before/after and deltas ----------------------------------\n",
    "comparison_report = {\n",
    "    \"Before\": first_pass_scores,\n",
    "    \"After\": {\n",
    "        \"SummarizationScore\": improved_scores[\"SummarizationScore\"],\n",
    "        \"CoherenceScore\": improved_scores[\"CoherenceScore\"],\n",
    "        \"TonalityScore\": improved_scores[\"TonalityScore\"],\n",
    "        \"SafetyScore\": improved_scores[\"SafetyScore\"],\n",
    "        \"CompositeScore\": improved_composite,\n",
    "    },\n",
    "    \"Delta\": {\n",
    "        \"SummarizationScore\": improved_scores[\"SummarizationScore\"] - first_pass_scores[\"SummarizationScore\"],\n",
    "        \"CoherenceScore\": improved_scores[\"CoherenceScore\"] - first_pass_scores[\"CoherenceScore\"],\n",
    "        \"TonalityScore\": improved_scores[\"TonalityScore\"] - first_pass_scores[\"TonalityScore\"],\n",
    "        \"SafetyScore\": improved_scores[\"SafetyScore\"] - first_pass_scores[\"SafetyScore\"],\n",
    "        \"CompositeScore\": improved_composite - first_pass_scores[\"CompositeScore\"],\n",
    "    },\n",
    "    \"ImprovedSummaryPreview\": improved_summary_text[:600] + (\"...\" if len(improved_summary_text) > 600 else \"\"),\n",
    "}\n",
    "\n",
    "print(\"===== Enhancement Results (Before vs After) =====\")\n",
    "print(json.dumps(comparison_report, indent=2))\n",
    "\n",
    "# Concise interpretation for the write-up\n",
    "print(\"\\n===== Interpretation =====\")\n",
    "print(\"A second pass used model-based rubric feedback to revise the summary and was re-evaluated with the same metrics.\")\n",
    "print(\"Gains in Summarization indicate better source faithfulness; regressions suggest overfitting to feedback or added hallucinations.\")\n",
    "print(\"Model-only self-critique is limited; for reliability, add source-grounded checks and stricter constraints on factual claims.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
